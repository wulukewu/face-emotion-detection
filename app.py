import streamlit as st
import cv2
import numpy as np
import joblib
import av
import pandas as pd
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, WebRtcMode
import os

# --- 1. Mac M3 é—œéµè¨­å®š (é˜²æ­¢ Mutex Lock å´©æ½°) ---
os.environ["OMP_NUM_THREADS"] = "1"

# --- 2. TensorFlow å¿…é ˆåœ¨è¨­å®šå®Œç’°å¢ƒè®Šæ•¸å¾ŒåŒ¯å…¥ ---
import tensorflow as tf
from tensorflow.keras.models import load_model

# åŒ¯å…¥ä½ çš„ç‰¹å¾µæå–å·¥å…·
import feature_extractor as fe

# --- 3. å…¨åŸŸé…ç½® (è§£æ±º ScriptRunContext è­¦å‘Šçš„é—œéµ) ---
# æˆ‘å€‘ç”¨é€™å€‹å­—å…¸ä¾†åœ¨ UI å’Œ èƒŒæ™¯åŸ·è¡Œç·’ ä¹‹é–“å‚³éè¨­å®š
if "system_config" not in st.session_state:
    st.session_state.system_config = {"model_type": "Traditional ML (HOG+RF)"}

# å®šç¾©ä¸€å€‹å…¨åŸŸè®Šæ•¸å¼•ç”¨ï¼Œè®“èƒŒæ™¯åŸ·è¡Œç·’ä¹Ÿèƒ½è®€åˆ°
SYSTEM_CONFIG = {"model_type": "Traditional ML (HOG+RF)"}

# --- 4. é é¢è¨­å®š ---
st.set_page_config(page_title="Emotion AI Dual-Core", page_icon="ğŸ§ ", layout="wide")

# --- 5. è¼‰å…¥æ¨¡å‹è³‡æº (å¿«å–åŠ é€Ÿ) ---
@st.cache_resource
def load_all_models():
    """
    ä¸€æ¬¡è¼‰å…¥æ‰€æœ‰æ¨¡å‹è³‡æº
    """
    resources = {}
    try:
        # A. è¼‰å…¥å‚³çµ±æ©Ÿå™¨å­¸ç¿’æ¨¡å‹
        if os.path.exists("emotion_model.joblib"):
            resources['rf_model'] = joblib.load("emotion_model.joblib")
            resources['scaler'] = joblib.load("feature_scaler.joblib")
        else:
            st.error("âš ï¸ æ‰¾ä¸åˆ° emotion_model.joblibï¼Œè«‹å…ˆåŸ·è¡Œ train_model.py")
            return None
        
        # B. è¼‰å…¥æ·±åº¦å­¸ç¿’æ¨¡å‹ (CNN)
        if os.path.exists("emotion_model_cnn.h5"):
            resources['cnn_model'] = load_model("emotion_model_cnn.h5")
        else:
            st.warning("âš ï¸ æ‰¾ä¸åˆ° emotion_model_cnn.h5 (CNN æ¨¡å‹)ï¼Œè«‹å…ˆåŸ·è¡Œ train_cnn.py")
            resources['cnn_model'] = None

        # C. è¼‰å…¥æ¨™ç±¤å°æ‡‰è¡¨
        if os.path.exists("label_map.joblib"):
            label_map = joblib.load("label_map.joblib")
            if isinstance(list(label_map.keys())[0], str):
                 resources['inv_map'] = {v: k for k, v in label_map.items()}
            else:
                 resources['inv_map'] = label_map 
        else:
            st.error("âš ï¸ æ‰¾ä¸åˆ° label_map.joblib")
            return None
             
        return resources
    except Exception as e:
        st.error(f"Error loading model files: {e}")
        return None

# åŸ·è¡Œè¼‰å…¥
resources = load_all_models()
label_map = resources['inv_map'] if resources else {}

# è¼‰å…¥äººè‡‰åµæ¸¬å™¨
try:
    cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
    face_cascade = cv2.CascadeClassifier(cascade_path)
except Exception as e:
    st.error(f"Error loading Face Detector: {e}")

# --- 6. æ ¸å¿ƒé æ¸¬å‡½å¼ (åŒ…å« Mac M3 ä¿®å¾©) ---
def predict_emotion(face_img, model_type):
    """
    æ ¹æ“šä½¿ç”¨è€…é¸æ“‡ï¼Œå°‡åœ–ç‰‡é€å¾€ä¸åŒçš„æ¨¡å‹
    """
    if not resources:
        return "Error", 0.0, {}

    # A. å‚³çµ±æ©Ÿå™¨å­¸ç¿’è·¯å¾‘
    if model_type == "Traditional ML (HOG+RF)":
        features = fe.preprocess_and_extract_features_single(face_img)
        features_scaled = resources['scaler'].transform(features)
        probs = resources['rf_model'].predict_proba(features_scaled)[0]

    # B. æ·±åº¦å­¸ç¿’ (CNN) è·¯å¾‘
    else:
        if resources['cnn_model'] is None:
            return "No Model", 0.0, {}

        img_resized = cv2.resize(face_img, (64, 64))
        # Normalize (é™¤ä»¥ 255) - é€™ä¸€æ­¥è¶…ç´šé‡è¦ï¼
        img_norm = img_resized.astype("float32") / 255.0
        img_input = img_norm.reshape(1, 64, 64, 1)
        
        # é æ¸¬ (Mac M3 é—œéµä¿®å¾©ï¼šå¼·åˆ¶ç”¨ CPU)
        with tf.device('/cpu:0'):
            probs = resources['cnn_model'].predict(img_input, verbose=0)[0]

    # å¾Œè™•ç†
    best_idx = np.argmax(probs)
    best_label = label_map[best_idx]
    best_conf = probs[best_idx]
    prob_dict = {label_map[i]: float(probs[i]) for i in range(len(probs))}
    
    return best_label, best_conf, prob_dict

# --- 7. å½±åƒè™•ç†é¡åˆ¥ (èƒŒæ™¯åŸ·è¡Œç·’) ---
class EmotionProcessor(VideoProcessorBase):
    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        try:
            img = frame.to_ndarray(format="bgr24")
            img = cv2.flip(img, 1)
            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            
            faces = face_cascade.detectMultiScale(img_gray, scaleFactor=1.1, minNeighbors=4, minSize=(30, 30))
            
            for (x, y, w, h) in faces:
                cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)
                
                try:
                    face_roi = img_gray[y:y+h, x:x+w]
                    
                    # ã€ä¿®æ­£ã€‘ç›´æ¥è®€å–å…¨åŸŸè®Šæ•¸
                    current_model = SYSTEM_CONFIG["model_type"]
                    
                    label, conf, _ = predict_emotion(face_roi, model_type=current_model)
                    
                    color = (0, 255, 0)
                    if label in ['Angry', 'Fear', 'Sad']: color = (0, 0, 255)
                    elif label == 'Happy': color = (0, 255, 255)
                    
                    cv2.rectangle(img, (x, y-30), (x+w, y), color, -1)
                    text = f"{label} ({int(conf*100)}%)"
                    cv2.putText(img, text, (x+5, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 2)
                    
                except Exception as inner_e:
                    print(f"Prediction Error: {inner_e}")
                    pass

            return av.VideoFrame.from_ndarray(img, format="bgr24")
            
        except Exception as e:
            print(f"Frame Processing Error: {e}")
            return frame

# --- 8. ä¸»ä»‹é¢ UI ---

st.title("ğŸ§  Face Emotion Detection System")
st.markdown("### Scikit-Learn vs TensorFlow comparison")

# --- å´é‚Šæ¬„ ---
with st.sidebar:
    st.header("âš™ï¸ Model Settings")
    
    model_choice = st.radio(
        "Choose AI Core:",
        ("Traditional ML (HOG+RF)", "Deep Learning (CNN)"),
        index=0
    )
    
    # ã€ä¿®æ­£ã€‘æ›´æ–°å…¨åŸŸè®Šæ•¸
    SYSTEM_CONFIG["model_type"] = model_choice
    
    st.divider()
    st.info(f"**Current Engine:**\n{model_choice}")
    
    if model_choice == "Traditional ML (HOG+RF)":
        st.caption("âœ… Fast Inference\nâœ… Explicit Features (LBP/HOG)\nâŒ Less Robust to lighting")
    else:
        st.caption("âœ… Deep Learning\nâœ… End-to-End Feature Learning\nâš ï¸ Running on CPU (Mac Optimization)")

# --- åˆ†é ä»‹é¢ ---
tab1, tab2 = st.tabs(["ğŸ“¸ Live Webcam", "ğŸ“‚ Upload Image"])

# --- TAB 1: å³æ™‚æ”å½±æ©Ÿ ---
with tab1:
    col1, col2 = st.columns([2, 1])
    with col1:
        st.subheader("Real-time Analysis")
        webrtc_streamer(
            key="emotion-live",
            mode=WebRtcMode.SENDRECV,
            video_processor_factory=EmotionProcessor,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )
    with col2:
        st.write("### Technical Details")
        st.write(f"**Active Model:** {model_choice}")
        if model_choice == "Deep Learning (CNN)":
            st.markdown("""
            - **Input:** 64x64 Normalized Pixels
            - **Architecture:** 3-Layer CNN
            - **Backend:** TensorFlow (CPU Mode)
            """)
        else:
            st.markdown("""
            - **Input:** HOG (Shape) + LBP (Texture)
            - **Algorithm:** Random Forest Classifier
            - **Backend:** Scikit-Learn
            """)

# --- TAB 2: åœ–ç‰‡ä¸Šå‚³ ---
with tab2:
    uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "png", "jpeg"])
    
    if uploaded_file is not None:
        file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
        img_bgr = cv2.imdecode(file_bytes, 1)
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        
        col_img, col_stats = st.columns(2)
        
        with col_img:
            st.image(img_rgb, caption="Uploaded Image", use_container_width=True)
            
        faces = face_cascade.detectMultiScale(img_gray, scaleFactor=1.1, minNeighbors=3, minSize=(30, 30))
        
        if len(faces) == 0:
            st.warning("âš ï¸ No specific face detected. Analyzing full image area.")
            face_roi = img_gray
        else:
            st.success(f"âœ… Face detected!")
            (x, y, w, h) = faces[0] 
            face_roi = img_gray[y:y+h, x:x+w]

        try:
            label, conf, prob_dict = predict_emotion(face_roi, model_type=model_choice)
            
            with col_stats:
                st.subheader(f"Results ({model_choice})")
                
                emoji_map = {"Happy": "ğŸ˜„", "Sad": "ğŸ˜¢", "Angry": "ğŸ˜¡", "Fear": "ğŸ˜±", "Surprise": "ğŸ˜²", "Neutral": "ğŸ˜"}
                emoji = emoji_map.get(label, "ğŸ˜")
                
                st.metric(label="Predicted Emotion", value=f"{emoji} {label}", delta=f"{conf*100:.1f}% Confidence")
                
                st.markdown("---")
                df_probs = pd.DataFrame(list(prob_dict.items()), columns=["Emotion", "Probability"])
                df_probs["Probability"] = df_probs["Probability"] * 100 
                df_probs = df_probs.set_index("Emotion")
                
                chart_color = "#FF4B4B" if model_choice == "Deep Learning (CNN)" else "#00CC96"
                st.bar_chart(df_probs, color=chart_color)
                
        except Exception as e:
            st.error(f"Prediction Error: {e}")